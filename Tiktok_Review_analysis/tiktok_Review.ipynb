{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud,STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset\n",
    "data_path = 'B:/_GITHUB/Data-Science-Projects/Tiktok_Review_analysis/dataset/tiktok_google_play_reviews.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()\n",
    "# print(data.head().to_markdown(index = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The dataset contains {} observations and {} features\".format(data.shape[0], data.shape[1]))\n",
    "print(\"The features are :  {} \".format(', '.join(data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains, 10 columns with different values some of which we would not use since we\n",
    "are analyzing the reviews on TikTok, the two crucials tables here would be be _\"score\"_ and \n",
    "_\"content\"_. Thus we can build a new dataset with those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['content','score']]\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby_score = data.groupby(['score'])\n",
    "# df_groupby_score.ngroups\n",
    "# df_groupby_score.groups\n",
    "# df_groupby_score.size()\n",
    "df_groupby_score.describe().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us an insight into the __score__ distribution value, we can observe the __count, unique and frequency__ associated with each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null value and drop any \n",
    "print('Before droping null values: \\n',data.isnull().sum())\n",
    "data = data.dropna()\n",
    "print('After droping null values: \\n',data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "stopword = set(stopwords.words(\"english\"))\n",
    "stemmer = nltk.SnowballStemmer(\"english\")\n",
    "# print(stopword)\n",
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text = \" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "data['content'] = data['content'].apply(preprocess)\n",
    "# print(data['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = data['score'].value_counts()\n",
    "index = ratings.index\n",
    "fig = px.pie(data, values=ratings, names=index, hole = 0.5)\n",
    "fig.update_layout(title = 'Ratings distribution', title_x=0.5, height= 500, width = 800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking most frequently used words\n",
    "text = ' '.join(content for content in data.content)\n",
    "# print(text)\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords = stopwords, background_color = 'black').generate(text)\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above picture shows the most frequently used words, we can see that the word *\"good\", \"best\" ...* have been used to describe the app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Sentiment analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code we are going to perform sentiment analysis, in order to categorize the review comments into various categories. To achieve this, we are going to use the *__Valence Aware Dictionary for sEntiment Reasoning, or Vader__*, is a NLP algorithm that blended a sentiment lexicon approach as well as grammatical rules and syntactical conventions for expressing sentiment polarity and intensity. Vader is an open-sourced package within the Natural Language Toolkit (NLTK).\n",
    "*__An example of the Vader is shown in the picture below:__*\n",
    "![](assets/vader.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to get the good and bad comments sorted\n",
    "# nltk.download(\"vader_lexicon\")\n",
    "sentiments = SentimentIntensityAnalyzer()\n",
    "data[\"Positive\"] = [sentiments.polarity_scores(i)[\"pos\"] for i in data[\"content\"]]\n",
    "data[\"Negative\"] = [sentiments.polarity_scores(i)[\"neg\"] for i in data[\"content\"]]\n",
    "data[\"Neutral\"] = [sentiments.polarity_scores(i)[\"neu\"] for i in data[\"content\"]]\n",
    "data = data[['content', 'Positive', 'Negative', 'Neutral']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_comments = ' '.join([i for i in data['content'][data['Positive'] > data['Negative']]])\n",
    "# print(positive_comments)\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords = stopwords, background_color = 'white').generate(positive_comments)\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_comments = ' '.join([i for i in data['content'][data['Positive'] < data['Negative']]])\n",
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(stopwords = stopwords, background_color = 'white', max_words = 100).generate(negative_comments)\n",
    "plt.figure(figsize = (10,15))\n",
    "plt.imshow(wordcloud, interpolation = 'bilinear')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "tiktok_mask = np.array(Image.open(\"./assets/tiktok.jpg\"))\n",
    "# tiktok_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_pixel_value(pixel):\n",
    "    if pixel == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return pixel\n",
    "\n",
    "new_tiktok_mask = np.ndarray((tiktok_mask.shape[0], tiktok_mask.shape[1]), np.int32)\n",
    "# print(new_tiktok_mask)\n",
    "\n",
    "# for i in range(len(tiktok_mask)):\n",
    "#     new_tiktok_mask[i] = list(map(transform_pixel_value, tiktok_mask[i]))\n",
    "    # new_tiktok_mask = map(transform_pixel_value, tiktok_mask[i])\n",
    "\n",
    "# new_tiktok_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(stopwords = stopwords, max_words = 100, mask = tiktok_mask, contour_width = 3, contour_color = \"black\")\n",
    "wc.generate(text)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "In this notebook, we analyzed what __TikTok__ users think of the app. We found good and bad comments and built a __wordcloud__\n",
    "of comments.  "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db816d7616de24ade41e083521320c478457ecc7c227d9955d702456c2841bc9"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
